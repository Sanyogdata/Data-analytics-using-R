library(tm)
install.packages("SnowballC")
library(SnowballC)
library(ggplot2)
library(wordcloud)
library(tm)

getwd()
sapio <- readLines("sapio.csv")
docs <- as.character(sapio)

  docs <- Corpus(VectorSource(sapio))
inspect(docs)
#inspect a particular document
writeLines(as.character(docs[[30]]))
#Start preprocessing
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ",
                                                                   x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, " -")
docs <- tm_map(docs, toSpace, " /")
docs <- tm_map(docs, toSpace, " NA")
writeLines(as.character(docs[[30]]))
#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list 
docs <- tm_map(docs, removeWords, stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))
#Need SnowballC library for stemming
library(SnowballC)
#Stem document
docs <- tm_map(docs,stemDocument)
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "encourag", replacement = "encourage")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "emphathi", replacement = "emphathy")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "awesom", replacement = "awesome")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "fantas", replacement = "fantastic")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "honesti", replacement = "honesty")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "psycholog", replacement = "pyschology")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "judg", replacement = "judge")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "theatr", replacement = "theatre")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "judg", replacement = "judge")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "somewher", replacement = "somewhere")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "superpow", replacement = "superpower")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "andgovern", replacement = "govern")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "inenterpris", replacement = "enterpris")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "team-", replacement = "team")
#inspect
writeLines(as.character(docs[[30]]))
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
#inspect segment of document term matrix
inspect(dtm[1:2,1000:1005])
#collapse matrix by summing over columns - this gets total counts (over all docs)
#for each term
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)

#create sort order (asc)
ord <- order(freq,decreasing=TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
#remove very frequent and very rare words
dtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(4, 20),
                                             bounds = list(global = c(3,27))))
freqr <- colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
#create sort order (asc)
ordr <- order(freqr,decreasing=TRUE)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect least frequently occurring terms
freqr[tail(ordr)]
#list most frequent terms. Lower bound specified as second argument
findFreqTerms(dtmr,lowfreq=10)
#correlations
findAssocs(dtmr,"loyal",0.5)
findAssocs(dtmr,"dating",0.6)
findAssocs(dtmr,"destination",0.6)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p <- ggplot(subset(wf, freqr>100), aes(term, occurrences))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
findAssocs(dtmr, 'loyal', 0.5);
#wordcloud
library(wordcloud)
#setting seed for consistent cloud
set.seed(42)
#limit words by specifying min frequency
wordcloud(names(freqr),freqr, min.freq=50)
#...add color
wordcloud(names(freqr),freqr,min.freq=50,colors=brewer.pal(6,"Dark2"))
